# Code reading

"Application Template", "Validation" or "System Performance" is generated by CubeMX following CubeMX components selection options.

## Application template

### Shape of input and output buffer (network.h)

```
#define AI_NETWORK_IN_NUM       (1)
#define AI_NETWORK_IN_1  \
  AI_BUFFER_OBJ_INIT(AI_BUFFER_FORMAT_FLOAT, 32, 32, 1, 1, NULL)
#define AI_NETWORK_IN_1_SIZE \
  (32 * 32 * 1)

#define AI_NETWORK_OUT_NUM      (1)
#define AI_NETWORK_OUT_1  \
  AI_BUFFER_OBJ_INIT(AI_BUFFER_FORMAT_FLOAT, 1, 1, 3, 1, NULL)
#define AI_NETWORK_OUT_1_SIZE \
  (1 * 1 * 3)
```

### Neural network (network.c)

My neural network is defined in the source code.

### Weights (network_data.c)

This is weights of the network.

```
ai_handle ai_network_data_weights_get(void)
{

  AI_ALIGNED(4)
  static const ai_u8 s_network_weights[ 23692 ] = {
    0x29, 0x4b, 0xe3, 0xbd, 0x1b, 0x90, 0x91, 0xbe, 0x98, 0xc3,
    0x54, 0xbe, 0xf3, 0x7a, 0xae, 0x3d, 0x59, 0x82, 0x6e, 0xbc,
    0xeb, 0x61, 0xa5, 0xbe, 0x69, 0x2d, 0xa9, 0xbd, 0xae, 0x03,
    0xac, 0xbe, 0x35, 0xc8, 0xb0, 0xbe, 0x96, 0xec, 0x05, 0xbe,
                         :
```

### Networks (app_x-cube-ai.c)

This is network entries definition as a constant.

```
static const ai_network_entry_t networks[AI_MNETWORK_NUMBER] = {
    {
        .name = (const char *)AI_NETWORK_MODEL_NAME,
        .config = AI_NETWORK_DATA_CONFIG,
        .ai_get_info = ai_network_get_info,
        .ai_create = ai_network_create,
        .ai_destroy = ai_network_destroy,
        .ai_get_error = ai_network_get_error,
        .ai_init = ai_network_init,
        .ai_run = ai_network_run,
        .ai_forward = ai_network_forward,
        .ai_data_weights_get_default = ai_network_data_weights_get,
        .params = { AI_NETWORK_DATA_WEIGHTS(0),
                AI_NETWORK_DATA_ACTIVATIONS(0)},
    },
};
```

### Basic APIs (app_x-cube-ai.c)

In my case, \*name is "network".

```
ai_error ai_mnetwork_create(const char *name, ai_handle* network,
        const ai_buffer* network_config);
        
ai_bool ai_mnetwork_init(ai_handle network, const ai_network_params* params);

ai_i32 ai_mnetwork_run(ai_handle network, const ai_buffer* input,
        ai_buffer* output);
```

### Buffer format (ai_platform.h)

This is a type definition of input and output buffers. As for width and size, I just follow the definitions in network.h.

```
/*!
 * @struct ai_buffer
 * @ingroup ai_platform
 * @brief Memory buffer storing data (optional) with a shape, size and type.
 * This datastruct is used also for network querying, where the data field may
 * may be NULL.
 */
typedef struct ai_buffer_ {
  ai_buffer_format        format;     /*!< buffer format */
  ai_u16                  n_batches;  /*!< number of batches in the buffer */
  ai_u16                  height;     /*!< buffer height dimension */
  ai_u16                  width;      /*!< buffer width dimension */
  ai_u32                  channels;   /*!< buffer number of channels */
  ai_handle               data;       /*!< pointer to buffer data */
} ai_buffer;
```

```
/*!
 * @enum buffer formats enum list
 * @ingroup ai_platform
 *
 * List supported data buffer types.
 */
enum {
  AI_BUFFER_FORMAT_NONE     = 0x00,
  AI_BUFFER_FORMAT_FLOAT    = 0x01,
  AI_BUFFER_FORMAT_U8       = 0x10,
  AI_BUFFER_FORMAT_Q7       = 0x31, 
  AI_BUFFER_FORMAT_Q15      = 0x32,
};
```

## How to run the network

Refer to "aiSystemPerformance.c" that is generated by selecting "System Performance" on CubeMX.

Maybe it is a good idea to use the code as a basis for developing an application. I modified the code by removing lines to make it simplest as possible, and the code became as follows (the normalization function should be modified depending on the use case):

```
#include <ai.h>
#include <bsp_ai.h>
#include "ai_platform.h"

#define AI_BUFFER_NULL(ptr_)  \
  AI_BUFFER_OBJ_INIT( \
    AI_BUFFER_FORMAT_NONE|AI_BUFFER_FMT_FLAG_CONST, \
    0, 0, 0, 0, \
    AI_HANDLE_PTR(ptr_))

static ai_u8 activations[AI_MNETWORK_DATA_ACTIVATIONS_SIZE];
ai_handle handle;
ai_network_report report;

// TOTO: mean value calculation
void normalize(ai_float *in_data, ai_float *normalized_data) {

	float sum_ = 0.0f;
	float mean_ = 0.0f;
	float max_ = 0.0f;
	float min_ = 80.0f;
	float range_;

	for (int i = 0; i < AI_MNETWORK_IN_1_SIZE; i++) {
		sum_ += in_data[i];
		if (in_data[i] > max_)
			max_ = in_data[i];
		if (in_data[i] < min_)
			min_ = in_data[i];
	}
	mean_ = (max_ + min_) / 2.0;
	range_ = (max_ - min_) / 2.0;
	for (int i = 0; i < AI_MNETWORK_IN_1_SIZE; i++) {
		normalized_data[i] = (in_data[i] - mean_) / range_;
	}
}

int ai_init(void)
{

    ai_error err;
    const char *nn_name;
    const ai_network_params params = {
            AI_BUFFER_NULL(NULL),
            AI_BUFFER_NULL(activations) };

    // Find a network
	nn_name = ai_mnetwork_find(NULL, 0);
	if (nn_name) {
		printf("\nFound network: \"%s\"\n", nn_name);
	} else {
        printf("E: ai_mnetwork_find\n");
		return -1;
	}

	// Create the network
    err = ai_mnetwork_create(nn_name, &handle, NULL);
    if (err.type) {
        printf("E: ai_mnetwork_create\n");
        return -1;
    }

    // Initialize the network
    if (!ai_mnetwork_init(handle, &params)) {
        printf("E: ai_mnetwork_init\n");
        return -1;
    }
    return 0;
}

int ai_infer(ai_float *input_data, const char *label)
{

    ai_buffer ai_input[1];
    ai_buffer ai_output[1];
    ai_float* output_;
	ai_float normalized_data[AI_MNETWORK_IN_1_SIZE];
	ai_float output_data[AI_MNETWORK_OUT_1_SIZE];

    // Normalize the input data
    normalize(input_data, normalized_data);

    ai_input[0] = report.inputs;
    ai_output[0] = report.outputs;

    ai_input[0].n_batches  = 1;
    ai_input[0].data = AI_HANDLE_PTR(normalized_data);
    ai_output[0].n_batches = 1;
    ai_output[0].data = AI_HANDLE_PTR(output_data);

	ai_mnetwork_run(handle, &ai_input[0], &ai_output[0]);

	output_ = (ai_float *) (ai_output[0].data);
	printf("[Inference] input: \"%s\",  result: [%d%%, %d%%, %d%%]\n",
			label,
			(int) (output_[0] * 100),
			(int) (output_[1] * 100),
			(int) (output_[2] * 100));

    return 0;
}
```
